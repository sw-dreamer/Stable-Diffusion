{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spPNhW_6YtKK"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision diffusers transformers accelerate safetensors pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APdvG2lbYtEL"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn pyngrok nest-asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNCwIlASYtBt"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLG4AjdqYs_I"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSueU9thY4HS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from langdetect import detect\n",
        "from starlette.responses import FileResponse\n",
        "import torch, gc\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import uvicorn\n",
        "from fastapi.responses import HTMLResponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVBHlDrpKIyE"
      },
      "outputs": [],
      "source": [
        "# 환경 변수 설정\n",
        "\n",
        "os.environ['HF_TOKEN'] = 'HUGGINGFACE TOKEN 값'\n",
        "os.environ['MODEL_ID_LLAMA'] = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "os.environ['MODEL_ID_SD'] = 'runwayml/stable-diffusion-v1-5'\n",
        "os.environ['DEVICE'] = 'cuda'\n",
        "\n",
        "HF_TOKEN = os.environ['HF_TOKEN']\n",
        "MODEL_ID_LLAMA = os.environ['MODEL_ID_LLAMA']\n",
        "MODEL_ID_SD = os.environ['MODEL_ID_SD']\n",
        "DEVICE = os.environ['DEVICE']\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"NGROK TOKEN 값\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBxfdT3sKKPm"
      },
      "outputs": [],
      "source": [
        "# 유틸 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def get_env(name, default=None):\n",
        "    return os.getenv(name, default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jX9MX2TKOUG"
      },
      "outputs": [],
      "source": [
        "class LlamaTranslator:\n",
        "    def __init__(self):\n",
        "        clear_memory()\n",
        "        self.model_id = MODEL_ID_LLAMA\n",
        "        self.token = HF_TOKEN\n",
        "        self.offload_dir = \"offload_8b\"\n",
        "        os.makedirs(self.offload_dir, exist_ok=True)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, use_auth_token=self.token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            use_auth_token=self.token,\n",
        "            device_map=\"auto\",\n",
        "            offload_folder=self.offload_dir,\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    def generate(self, system_message, user_message):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        eot_id = self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "        terminators = [self.tokenizer.eos_token_id]\n",
        "        if eot_id is not None:\n",
        "            terminators.append(eot_id)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=256,\n",
        "            eos_token_id=terminators if len(terminators) > 1 else terminators[0],\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        response = outputs[0][input_ids.shape[-1]:]\n",
        "        return self.tokenizer.decode(response, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iesibouWLbDK"
      },
      "outputs": [],
      "source": [
        "class ImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_id = MODEL_ID_SD\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def generate_image(self, prompt, height=512, width=512, guidance_scale=8):\n",
        "        clear_memory()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            self.model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        ).to(self.device)\n",
        "        pipe.enable_attention_slicing()\n",
        "        pipe.enable_vae_slicing()\n",
        "\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            result = pipe(prompt, height=height, width=width, guidance_scale=guidance_scale)\n",
        "\n",
        "        image = result.images[0]\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        file_name = f\"generated_image_{timestamp}.jpg\"\n",
        "        image.save(file_name)\n",
        "\n",
        "        del pipe\n",
        "        clear_memory()\n",
        "        return image, file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mlmpSP_KWI9"
      },
      "outputs": [],
      "source": [
        "app = FastAPI(title=\"AI Image Generator API\", version=\"1.0\")\n",
        "\n",
        "class PromptRequest(BaseModel):\n",
        "    prompt_text: str\n",
        "\n",
        "# HTML 폼 제공\n",
        "@app.get(\"/generate\", response_class=HTMLResponse)\n",
        "def generate_form():\n",
        "    html_content = \"\"\"\n",
        "    <html>\n",
        "        <head>\n",
        "            <title>AI Image Generator</title>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h2>AI Image Generator</h2>\n",
        "            <form id=\"generate-form\">\n",
        "                <label>Prompt:</label><br>\n",
        "                <input type=\"text\" id=\"prompt_text\" size=\"50\"><br><br>\n",
        "                <button type=\"button\" onclick=\"submitPrompt()\">Generate Image</button>\n",
        "            </form>\n",
        "            <h3>Result:</h3>\n",
        "            <div id=\"result\"></div>\n",
        "\n",
        "            <script>\n",
        "                async function submitPrompt() {\n",
        "                    const prompt = document.getElementById(\"prompt_text\").value;\n",
        "                    const response = await fetch(\"/api/generate\", {\n",
        "                        method: \"POST\",\n",
        "                        headers: {\"Content-Type\": \"application/json\"},\n",
        "                        body: JSON.stringify({prompt_text: prompt})\n",
        "                    });\n",
        "                    const data = await response.json();\n",
        "                    document.getElementById(\"result\").innerHTML = \n",
        "                        \"<b>Translated Prompt:</b> \" + data.translated_prompt + \"<br>\" +\n",
        "                        \"<b>Image Path:</b> <a href='\" + data.image_path + \"' target='_blank'>View Image</a>\";\n",
        "                }\n",
        "            </script>\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_content\n",
        "\n",
        "@app.post(\"/api/generate\")\n",
        "def generate_image_api(req: PromptRequest):\n",
        "    prompt_text = req.prompt_text\n",
        "    try:\n",
        "        lang = detect(prompt_text)\n",
        "    except Exception:\n",
        "        lang = \"unknown\"\n",
        "\n",
        "    translator = LlamaTranslator()\n",
        "\n",
        "    # 한국어 입력 시: 번역 + 세부 묘사 강화\n",
        "    if lang == \"ko\":\n",
        "        system_message = (\n",
        "            \"You are a professional prompt engineer and creative translator. \"\n",
        "            \"When given a Korean description, translate it into a vivid and imaginative English prompt \"\n",
        "            \"that expands the original meaning with rich visual details, atmosphere, lighting, and artistic style. \"\n",
        "            \"Optimize every prompt for high-quality wallpaper creation in Stable Diffusion.\"\n",
        "        )\n",
        "        translated_prompt = translator.generate(system_message, prompt_text)\n",
        "\n",
        "    # 영어 입력 시: 번역 없이 wallpaper 스타일로 확장\n",
        "    else:\n",
        "        system_message = (\n",
        "            \"You are a professional Stable Diffusion prompt engineer. \"\n",
        "            \"Enhance the given English prompt into a natural and visually rich version \"\n",
        "            \"suitable for a cinematic, high-quality wallpaper. \"\n",
        "            \"Add realistic details about lighting, color tones, composition, and atmosphere \"\n",
        "            \"while keeping the original meaning intact.\"\n",
        "        )\n",
        "        translated_prompt = translator.generate(system_message, prompt_text)\n",
        "\n",
        "    generator = ImageGenerator()\n",
        "    image, file_path = generator.generate_image(translated_prompt)\n",
        "\n",
        "    return {\n",
        "        \"original_prompt\": prompt_text,\n",
        "        \"translated_prompt\": translated_prompt,\n",
        "        \"image_path\": file_path\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0SSPYP5VRqQ"
      },
      "outputs": [],
      "source": [
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWMePNi8Xr1Z"
      },
      "outputs": [],
      "source": [
        "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "asyncio.get_event_loop().run_until_complete(server.serve())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
